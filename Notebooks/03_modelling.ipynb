{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f7cd2bc",
   "metadata": {},
   "source": [
    "# 03 Model Improvement and Time-Based Validation (Telecom Churn / Cease)\n",
    "\n",
    "This notebook trains **leakage-safe churn models** for the business objective:\n",
    "\n",
    "> **Prioritise retention resources by calling customers most likely to place a cease in the next 30 days.**\n",
    "\n",
    "## Objectives\n",
    "- Load feature dataset from Notebook 02\n",
    "- Use a **time-based train / validation / test split**\n",
    "- Train baseline models (**Logistic Regression** + **Random Forest**)\n",
    "- Evaluate with classification and business metrics\n",
    "- Optimise thresholds for **Top K% retention capacity**\n",
    "- Create **gains / lift** charts\n",
    "- Explain drivers (feature importance + SHAP if available)\n",
    "- Score external/future snapshot datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ddf2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, classification_report,\n",
    "    precision_recall_curve\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78bac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "cwd = Path.cwd()\n",
    "repo_dir = cwd.parent if cwd.name.lower() in {\"notebook\", \"notebooks\"} else cwd\n",
    "\n",
    "features_path_default = repo_dir / \"outputs\" / \"features\" / \"telecom_churn_model_features_v1.parquet\"\n",
    "features_path_csv = repo_dir / \"outputs\" / \"features\" / \"telecom_churn_model_features_v1.csv\"\n",
    "features_path_sample = Path(\"/mnt/data/telecom_churn_model_features_v1.parquet\")  # optional if copied here manually\n",
    "\n",
    "if features_path_default.exists():\n",
    "    features_path = features_path_default\n",
    "elif features_path_csv.exists():\n",
    "    features_path = features_path_csv\n",
    "elif features_path_sample.exists():\n",
    "    features_path = features_path_sample\n",
    "else:\n",
    "    raise FileNotFoundError(\"Feature dataset not found. Run Notebook 02 first.\")\n",
    "\n",
    "print(\"Loading features from:\", features_path)\n",
    "if features_path.suffix.lower() == \".parquet\":\n",
    "    df = pd.read_parquet(features_path)\n",
    "else:\n",
    "    df = pd.read_csv(features_path)\n",
    "\n",
    "print(df.shape)\n",
    "display(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907aa2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data prep and sanitisation\n",
    "df[\"snapshot_date\"] = pd.to_datetime(df[\"snapshot_date\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"snapshot_date\"]).copy()\n",
    "\n",
    "# Enforce target type\n",
    "df[\"target_cease_30d\"] = pd.to_numeric(df[\"target_cease_30d\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# Optional: drop highly leaky or identifier columns from modelling features\n",
    "id_cols = [\"unique_customer_identifier\", \"snapshot_date\"]\n",
    "target_col = \"target_cease_30d\"\n",
    "\n",
    "# Remove extreme hold ratios if present (cap to reduce outlier distortion)\n",
    "for c in [\"avg_hold_ratio_30d\", \"max_hold_ratio_30d\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        upper = df[c].quantile(0.99)\n",
    "        df[c] = df[c].clip(upper=upper)\n",
    "\n",
    "# Convert obvious numeric columns safely\n",
    "for c in df.columns:\n",
    "    if c in id_cols + [target_col]:\n",
    "        continue\n",
    "    # leave objects/categoricals as-is; convert only if likely numeric but stored as text\n",
    "    if df[c].dtype == \"object\":\n",
    "        # keep as categorical if too many non-numeric values\n",
    "        converted = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        # if at least 80% values become numeric, treat as numeric\n",
    "        if converted.notna().mean() >= 0.8:\n",
    "            df[c] = converted\n",
    "\n",
    "print(\"Date range:\", df[\"snapshot_date\"].min(), \"to\", df[\"snapshot_date\"].max())\n",
    "print(\"Target rate:\", round(df[target_col].mean()*100, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7b8d88",
   "metadata": {},
   "source": [
    "## 1) Time-based train / validation / test split\n",
    "\n",
    "We split by **snapshot_date** (not random split) to match real deployment.\n",
    "\n",
    "- **Train**: oldest period\n",
    "- **Validation**: middle period\n",
    "- **Test**: most recent period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb78c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time split by quantiles (works on sample and full data)\n",
    "q1 = df[\"snapshot_date\"].quantile(0.70)\n",
    "q2 = df[\"snapshot_date\"].quantile(0.85)\n",
    "\n",
    "train_df = df[df[\"snapshot_date\"] <= q1].copy()\n",
    "valid_df = df[(df[\"snapshot_date\"] > q1) & (df[\"snapshot_date\"] <= q2)].copy()\n",
    "test_df  = df[df[\"snapshot_date\"] > q2].copy()\n",
    "\n",
    "print(\"Train:\", train_df.shape, train_df[\"snapshot_date\"].min(), \"->\", train_df[\"snapshot_date\"].max())\n",
    "print(\"Valid:\", valid_df.shape, valid_df[\"snapshot_date\"].min(), \"->\", valid_df[\"snapshot_date\"].max())\n",
    "print(\"Test :\", test_df.shape,  test_df[\"snapshot_date\"].min(), \"->\", test_df[\"snapshot_date\"].max())\n",
    "\n",
    "display(pd.DataFrame({\n",
    "    \"split\": [\"train\",\"valid\",\"test\"],\n",
    "    \"rows\": [len(train_df), len(valid_df), len(test_df)],\n",
    "    \"target_rate_pct\": [train_df[target_col].mean()*100, valid_df[target_col].mean()*100, test_df[target_col].mean()*100]\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b31e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features used for modelling\n",
    "drop_cols = id_cols + [target_col]\n",
    "\n",
    "X_train = train_df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "y_train = train_df[target_col]\n",
    "\n",
    "X_valid = valid_df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "y_valid = valid_df[target_col]\n",
    "\n",
    "X_test = test_df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "# Identify numeric vs categorical columns\n",
    "numeric_cols = X_train.select_dtypes(include=[\"number\", \"bool\"]).columns.tolist()\n",
    "categorical_cols = [c for c in X_train.columns if c not in numeric_cols]\n",
    "\n",
    "print(\"Numeric cols:\", len(numeric_cols))\n",
    "print(\"Categorical cols:\", len(categorical_cols))\n",
    "print(\"Sample categorical:\", categorical_cols[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b6d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a86d89",
   "metadata": {},
   "source": [
    "## 2) Train baseline models\n",
    "\n",
    "We start with:\n",
    "- **Logistic Regression** (interpretable baseline)\n",
    "- **Random Forest** (non-linear baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3f030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline models\n",
    "logreg_pipe = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"model\", LogisticRegression(max_iter=1000, class_weight=\"balanced\", n_jobs=None))\n",
    "])\n",
    "\n",
    "rf_pipe = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced_subsample\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "logreg_pipe.fit(X_train, y_train)\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Validation predictions\n",
    "valid_scores_lr = logreg_pipe.predict_proba(X_valid)[:, 1]\n",
    "valid_scores_rf = rf_pipe.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "def metric_summary(y_true, y_score, model_name):\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"roc_auc\": roc_auc_score(y_true, y_score) if len(np.unique(y_true)) > 1 else np.nan,\n",
    "        \"pr_auc\": average_precision_score(y_true, y_score) if len(np.unique(y_true)) > 1 else np.nan,\n",
    "        \"base_rate\": float(np.mean(y_true))\n",
    "    }\n",
    "\n",
    "metrics_df = pd.DataFrame([\n",
    "    metric_summary(y_valid, valid_scores_lr, \"LogisticRegression\"),\n",
    "    metric_summary(y_valid, valid_scores_rf, \"RandomForest\")\n",
    "])\n",
    "display(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4159bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick best baseline by validation ROC-AUC (fallback to RF on tie/nan)\n",
    "if metrics_df[\"roc_auc\"].notna().any():\n",
    "    best_name = metrics_df.sort_values(\"roc_auc\", ascending=False)[\"model\"].iloc[0]\n",
    "else:\n",
    "    best_name = \"RandomForest\"\n",
    "\n",
    "best_model = logreg_pipe if best_name == \"LogisticRegression\" else rf_pipe\n",
    "print(\"Selected model:\", best_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2300d7",
   "metadata": {},
   "source": [
    "## 3) Threshold optimisation for retention capacity (Top K%)\n",
    "\n",
    "The business can only call a **limited proportion** of customers.  \n",
    "So we optimise on **Top K% prioritisation** rather than a fixed 0.5 threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e25c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-K evaluation helper\n",
    "def topk_metrics(y_true, scores, k_frac=0.1):\n",
    "    n = len(scores)\n",
    "    k = max(1, int(np.floor(n * k_frac)))\n",
    "    order = np.argsort(-scores)\n",
    "    top_idx = order[:k]\n",
    "\n",
    "    y_true = np.asarray(y_true)\n",
    "    top_y = y_true[top_idx]\n",
    "\n",
    "    captured = top_y.sum()\n",
    "    total_positives = y_true.sum()\n",
    "    recall_at_k = captured / total_positives if total_positives > 0 else np.nan\n",
    "    precision_at_k = top_y.mean() if len(top_y) > 0 else np.nan\n",
    "    lift_at_k = (precision_at_k / y_true.mean()) if y_true.mean() > 0 else np.nan\n",
    "    threshold = scores[order[k-1]] if k > 0 else 1.0\n",
    "\n",
    "    return {\n",
    "        \"k_frac\": k_frac,\n",
    "        \"k_customers\": k,\n",
    "        \"threshold\": float(threshold),\n",
    "        \"precision_at_k\": float(precision_at_k),\n",
    "        \"recall_at_k\": float(recall_at_k),\n",
    "        \"lift_at_k\": float(lift_at_k)\n",
    "    }\n",
    "\n",
    "# Evaluate multiple capacity levels on validation\n",
    "valid_scores_best = best_model.predict_proba(X_valid)[:, 1]\n",
    "topk_grid = [0.01, 0.02, 0.05, 0.10, 0.15, 0.20, 0.30]\n",
    "topk_valid_df = pd.DataFrame([topk_metrics(y_valid.values, valid_scores_best, k) for k in topk_grid])\n",
    "display(topk_valid_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ee2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose operational capacity (edit based on business capacity)\n",
    "RETENTION_TOP_K = 0.10  # e.g., call top 10% highest-risk customers\n",
    "\n",
    "chosen = topk_valid_df.loc[(topk_valid_df[\"k_frac\"] - RETENTION_TOP_K).abs().idxmin()].to_dict()\n",
    "score_threshold = chosen[\"threshold\"]\n",
    "\n",
    "print(f\"Chosen Top K% capacity: {chosen['k_frac']*100:.0f}%\")\n",
    "print(f\"Validation score threshold: {score_threshold:.4f}\")\n",
    "print(f\"Precision@K: {chosen['precision_at_k']:.3f} | Recall@K: {chosen['recall_at_k']:.3f} | Lift@K: {chosen['lift_at_k']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f8a6ed",
   "metadata": {},
   "source": [
    "## 4) Final evaluation on the test set (most recent period)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b6ced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score test set\n",
    "test_scores = best_model.predict_proba(X_test)[:, 1]\n",
    "test_pred_topk = (test_scores >= score_threshold).astype(int)\n",
    "\n",
    "# Standard metrics\n",
    "test_metrics = pd.DataFrame([metric_summary(y_test, test_scores, f\"{best_name} (test)\")])\n",
    "display(test_metrics)\n",
    "\n",
    "# Capacity-based metric at chosen threshold\n",
    "test_topk = topk_metrics(y_test.values, test_scores, RETENTION_TOP_K)\n",
    "display(pd.DataFrame([test_topk]))\n",
    "\n",
    "print(\"\\nClassification report using Top-K-derived threshold:\")\n",
    "print(classification_report(y_test, test_pred_topk, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391247b6",
   "metadata": {},
   "source": [
    "## 5) Business-ready gains and lift charts\n",
    "\n",
    "These charts show how well the model helps the business prioritise calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c1949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gains_lift_table(y_true, scores, n_bins=10):\n",
    "    dfp = pd.DataFrame({\"y\": np.asarray(y_true), \"score\": np.asarray(scores)}).sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "    dfp[\"decile\"] = pd.qcut(dfp.index + 1, q=n_bins, labels=False) + 1  # 1=highest score bucket\n",
    "    # convert so decile 1 is top bucket already\n",
    "    grp = dfp.groupby(\"decile\", as_index=False).agg(\n",
    "        customers=(\"y\",\"size\"),\n",
    "        churners=(\"y\",\"sum\"),\n",
    "        avg_score=(\"score\",\"mean\")\n",
    "    )\n",
    "    grp[\"cum_customers\"] = grp[\"customers\"].cumsum()\n",
    "    grp[\"cum_churners\"] = grp[\"churners\"].cumsum()\n",
    "    total_customers = grp[\"customers\"].sum()\n",
    "    total_churners = grp[\"churners\"].sum()\n",
    "    base_rate = total_churners / total_customers if total_customers > 0 else np.nan\n",
    "    grp[\"cum_customer_pct\"] = grp[\"cum_customers\"] / total_customers\n",
    "    grp[\"cum_churn_capture_pct\"] = grp[\"cum_churners\"] / total_churners if total_churners > 0 else np.nan\n",
    "    grp[\"precision\"] = grp[\"churners\"] / grp[\"customers\"]\n",
    "    grp[\"lift\"] = grp[\"precision\"] / base_rate if base_rate > 0 else np.nan\n",
    "    return grp\n",
    "\n",
    "gl_valid = gains_lift_table(y_valid, valid_scores_best, n_bins=10)\n",
    "gl_test = gains_lift_table(y_test, test_scores, n_bins=10)\n",
    "\n",
    "display(gl_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d694654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot gains chart (test)\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "ax.plot([0,1], [0,1], linestyle=\"--\")\n",
    "ax.plot(gl_test[\"cum_customer_pct\"], gl_test[\"cum_churn_capture_pct\"], marker=\"o\")\n",
    "ax.set_title(\"Gains Chart (Test)\")\n",
    "ax.set_xlabel(\"Cumulative % Customers Contacted\")\n",
    "ax.set_ylabel(\"Cumulative % Churners Captured\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot lift by decile (test)\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "ax.bar(gl_test[\"decile\"].astype(str), gl_test[\"lift\"])\n",
    "ax.set_title(\"Lift by Decile (Test)\")\n",
    "ax.set_xlabel(\"Decile (1 = highest risk)\")\n",
    "ax.set_ylabel(\"Lift vs Average\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a048e0",
   "metadata": {},
   "source": [
    "## 6) Explainability: feature importance (and SHAP if available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f73a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature names after preprocessing\n",
    "prep_fitted = best_model.named_steps[\"prep\"]\n",
    "feature_names = prep_fitted.get_feature_names_out()\n",
    "\n",
    "model_obj = best_model.named_steps[\"model\"]\n",
    "\n",
    "# Model-specific feature importance\n",
    "if hasattr(model_obj, \"feature_importances_\"):\n",
    "    importances = model_obj.feature_importances_\n",
    "    imp_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
    "elif hasattr(model_obj, \"coef_\"):\n",
    "    coefs = model_obj.coef_[0]\n",
    "    imp_df = pd.DataFrame({\"feature\": feature_names, \"importance\": np.abs(coefs), \"coef\": coefs}).sort_values(\"importance\", ascending=False)\n",
    "else:\n",
    "    imp_df = pd.DataFrame(columns=[\"feature\",\"importance\"])\n",
    "\n",
    "display(imp_df.head(30))\n",
    "\n",
    "# Plot top features\n",
    "topn = 20\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "top = imp_df.head(topn).sort_values(\"importance\", ascending=True)\n",
    "ax.barh(top[\"feature\"], top[\"importance\"])\n",
    "ax.set_title(f\"Top {topn} Model Drivers ({best_name})\")\n",
    "ax.set_xlabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d22f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional SHAP (if installed). Safe to skip if not available.\n",
    "try:\n",
    "    import shap\n",
    "    # Sample a subset for speed\n",
    "    X_valid_sample = X_valid.sample(min(500, len(X_valid)), random_state=42)\n",
    "    X_valid_tx = prep_fitted.transform(X_valid_sample)\n",
    "\n",
    "    if hasattr(model_obj, \"feature_importances_\"):  # tree model\n",
    "        explainer = shap.TreeExplainer(model_obj)\n",
    "        shap_values = explainer.shap_values(X_valid_tx)\n",
    "        # Binary classifier shap can be list or array depending on version\n",
    "        sv = shap_values[1] if isinstance(shap_values, list) else shap_values\n",
    "        shap.summary_plot(sv, X_valid_tx, feature_names=feature_names, show=True)\n",
    "    else:\n",
    "        print(\"SHAP tree summary skipped (selected model is not tree-based).\")\n",
    "except Exception as e:\n",
    "    print(\"SHAP not available or skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2535963",
   "metadata": {},
   "source": [
    "## 7) Score external / future snapshot datasets\n",
    "\n",
    "Use the same **feature engineering logic from Notebook 02** to build a feature table for new snapshots, then score with the trained model.\n",
    "\n",
    "The output should include:\n",
    "- `unique_customer_identifier`\n",
    "- `snapshot_date`\n",
    "- `churn_score`\n",
    "- `priority_rank`\n",
    "- `priority_band` (High / Medium / Low)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e5c924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example scoring output on test split (acts like external scoring format)\n",
    "scored = test_df[[\"unique_customer_identifier\", \"snapshot_date\", \"target_cease_30d\"]].copy()\n",
    "scored[\"churn_score\"] = test_scores\n",
    "scored = scored.sort_values(\"churn_score\", ascending=False).reset_index(drop=True)\n",
    "scored[\"priority_rank\"] = np.arange(1, len(scored) + 1)\n",
    "scored[\"priority_pct\"] = scored[\"priority_rank\"] / len(scored)\n",
    "\n",
    "# Example priority bands based on rank\n",
    "scored[\"priority_band\"] = np.select(\n",
    "    [scored[\"priority_pct\"] <= 0.10, scored[\"priority_pct\"] <= 0.30],\n",
    "    [\"High\", \"Medium\"],\n",
    "    default=\"Low\"\n",
    ")\n",
    "\n",
    "display(scored.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50025ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save outputs for business / ops handoff\n",
    "out_dir = repo_dir / \"outputs\" / \"model_outputs\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save metrics and scored outputs\n",
    "metrics_df.to_csv(out_dir / \"validation_model_metrics.csv\", index=False)\n",
    "topk_valid_df.to_csv(out_dir / \"validation_topk_metrics.csv\", index=False)\n",
    "test_metrics.to_csv(out_dir / \"test_model_metrics.csv\", index=False)\n",
    "gl_test.to_csv(out_dir / \"test_gains_lift_table.csv\", index=False)\n",
    "scored.to_csv(out_dir / \"retention_prioritisation_scores_test_example.csv\", index=False)\n",
    "\n",
    "print(\"Saved to:\", out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2800190",
   "metadata": {},
   "source": [
    "## Business interpretation guide (use in your presentation)\n",
    "\n",
    "- **Model score** ranks customers by likelihood to cease in next 30 days.\n",
    "- **Top K% threshold** maps directly to call-centre capacity.\n",
    "- **Recall@K** shows what % of likely churners you capture when only calling top K%.\n",
    "- **Lift** shows how much better the prioritised list performs vs calling customers at random.\n",
    "\n",
    "This is exactly what the business asked for: **better prioritisation of retention resources**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
